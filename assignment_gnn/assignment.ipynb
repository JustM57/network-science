{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment — Graph neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dgl.nn import GATConv, SAGEConv\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import clear_output\n",
    "from zlib import adler32\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from dgl import function as fn\n",
    "import pandas as pd\n",
    "from dgl.data import CoraGraphDataset\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Graph Convolutional Network (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task, we will train the Graph Convolutional Network (GCN) model to predict the node label (node classification task). Concretely, we will predict the category of the paper in the citation graph. First, let us build the cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoraGraphDataset(verbose=False)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains one graph – CORA. Next, load the feature matrix — 0/1-valued matrix of presence 1433 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_cora = dataset[0]\n",
    "feat = dgl_cora.ndata['feat']\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And node labels (here are 7 node classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dgl_cora.ndata['label']\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create train and test boolean mask across data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = dgl_cora.ndata['train_mask']\n",
    "test_mask = dgl_cora.ndata['test_mask']\n",
    "train_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in-degree nodes will lead to invalid output value. This is because no message will be passed to those nodes, the aggregation function will be appied on empty input. A common practice to avoid this is to add a self-loop for each node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_cora = dataset[0].add_self_loop()\n",
    "adj = dgl_cora.adj().to_dense()\n",
    "adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the GCN [Kipf, Welling, 2016]. The next node hidden state in the graph convolution is defined as follows:\n",
    "\n",
    "$$h_i^{l+1} = \\sigma\\left(b + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{l}W\\right)$$\n",
    "\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $c_{ij}$ is the product of the square root of node degrees (i.e.,  $c_{ij} = \\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$), and $\\sigma$ is an activation function. In the matrix form:\n",
    "\n",
    "$$H^{l+1} = \\sigma(b + D^{-1/2}AD^{-1/2}H^{l}W)$$\n",
    "\n",
    "where $D$ is diagonal degree matrix.\n",
    "\n",
    "Write a class `GCNLayer`. A function `init` takes an input and output dimensions (i.e. the size of matrix `W`) and store the linear layer, a function `forward` takes an adjacency matrix, node features and return node hidden states after the convolution.\n",
    "\n",
    "*Hint:*\n",
    "* *diagonal matrix can be inverted by direct inverting each element on diagonal*\n",
    "* *to speed up the calculation, compute a normalized adjacency using Hadamard product instead of dot product*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e138ae09c597b6c7265d780b1634606",
     "grade": false,
     "grade_id": "cell-49c0a67459fef8d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, adj, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "463c5f10e99dfd19ff264d98ded5c375",
     "grade": true,
     "grade_id": "cell-3773092e16e3398b",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "layer = GCNLayer(in_dim=100, out_dim=10)\n",
    "with torch.no_grad():\n",
    "    assert layer(adj, torch.randn(2708, 100)).shape == (2708, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `GCN` that represents a model for node classification among 7 classes. Let two `GCNLayer` layers with dimensions $\\text{in dim} \\to \\text{hid dim} \\to \\text{out dim}$. Apply `F.relu` as an activation function after the first layer. Let us select network dimensions as `in_dim=1433`, `hid_dim=16`, `out_dim=7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4da0da86b0292bdf9aeb703bf39cedd",
     "grade": false,
     "grade_id": "cell-5e0c138f5ccaea0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "\n",
    "    def forward(self, adj, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2817202cab5658c99e2757770dce56a1",
     "grade": true,
     "grade_id": "cell-fe037acfca3092bc",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCN(1433, 16, 7)\n",
    "with torch.no_grad():\n",
    "    assert model(adj, feat).shape == (2708, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks we will use `CrossEntropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wtrite a function train that takes a model, optimizer, loss object, adjacency matrix, features, labels, and then optimize the model. It returns loss values across training and test sets.\n",
    "\n",
    "_Hints:_\n",
    "* _to speed up the calculation test loss, use `torch.no_grad()`_\n",
    "* _you can detach the loss value as follows:_\n",
    "```\n",
    "loss = ce_loss(input, target)\n",
    "value = loss.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss, adj, feat, label):\n",
    "    logits = model.forward(adj, feat)\n",
    "    train_loss = ce_loss(logits[train_mask], label[train_mask])\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "    test_loss = ce_loss(logits[test_mask], label[test_mask])\n",
    "    return train_loss.item(), test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92e8f8bb90dd6b638949f87b76466709",
     "grade": true,
     "grade_id": "cell-4107fa691bec8865",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCN(1433, 16, 7)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train_loss, test_loss = train(model, opt, ce_loss, adj, feat, label)\n",
    "assert 1.9 < train_loss < 2\n",
    "assert 1.9 < test_loss < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(1433, 16, 7)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 200\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    train_loss, test_loss = train(model, opt, ce_loss, adj, feat, label)\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "953de62aa0c7dddea28b81206d4b8a3c",
     "grade": true,
     "grade_id": "cell-0dbd2da971a339fe",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(adj, feat)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(label[test_mask], y_pred)\n",
    "assert score > 0.75\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Graph Autoencoder (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous task, we have trained our model with supervised loss in the node classification task. One can train GNN in unsupervised fashion. To do so we can state our problem as a graph autoencoder. We will train embeddings in the way to reconstruct adjacency matrix.\n",
    "\n",
    "We will decode our adjacency matrix with `DotProductDecoder` class. A `forward` function computes dot product of node embeddings so that the result is a $n \\times n$ matrix where $n$ is the number of nodes and applies the sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4104bd3079d67b88bd5964bba6a145a7",
     "grade": false,
     "grade_id": "cell-31af15ac93053cc6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DotProductDecoder(nn.Module):\n",
    "    def __init__(self, dropout_rate=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate else None\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, h):\n",
    "        h = self.dropout(h) if self.dropout else h\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "331df56d58d8062e3c5f16b4e27f2877",
     "grade": true,
     "grade_id": "cell-5bd597fc40f459f5",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decoder = DotProductDecoder()\n",
    "pred_adj = decoder(torch.Tensor(np.arange(10).reshape(5, 2)))\n",
    "assert round(float(pred_adj.numpy()[0, 0]), 4) == 0.7311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `ReconstructionLoss` computes the difference between the initial adjacency matrix and predicted adjacency matrix by cross entropy. \n",
    "\n",
    "`__init__` function stores the adjacency matrix of the graph as a dense tensor, and stores the positive weight as $(N^2 - E) / E$, where $N$ is a number of nodes and $E$ is a number of edges. It is important in a case of sparse graphs since we have highly imbalances classes: edges (positive class) and pairs of nodes with no edges (negative class).\n",
    "\n",
    "`__call__` function calculates `F.binary_cross_entropy_with_logits` using positive weight `pos_weight` between the predicted adjacency matrix and real adjacency matrix.\n",
    "\n",
    "*Hint: to obtain dense adjacency matrix, use `g.adj().to_dense()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4983e4aeb2e2ca974d9deb827250bda",
     "grade": false,
     "grade_id": "cell-f21ed6c1f82b2b7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReconstructionLoss:\n",
    "    def __init__(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, predicted_adjacency):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44d3ee24596d02e5d721e94a20b3af26",
     "grade": true,
     "grade_id": "cell-dc7a5b14910f1b1b",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n = dgl_cora.number_of_nodes()\n",
    "pred_adj = torch.zeros(n, n)\n",
    "rec_loss = ReconstructionLoss(dgl_cora)\n",
    "assert round(rec_loss(pred_adj).item(), 2) == 1.38\n",
    "pred_adj = torch.ones(n, n)\n",
    "assert round(rec_loss(pred_adj).item(), 2) == 1.62\n",
    "pred_adj = dgl_cora.adj().to_dense()\n",
    "assert round(rec_loss(pred_adj).item(), 2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the quality the classification on an untrained GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = GCN(1433, 32, 16)\n",
    "model.eval()\n",
    "X = model.forward(adj, feat).data.numpy()\n",
    "y = label.numpy()\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X[train_mask], y[train_mask])\n",
    "\n",
    "y_true = y[test_mask]\n",
    "y_pred = lr.predict(X[test_mask])\n",
    "score = balanced_accuracy_score(y_true, y_pred)\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop so that node embeddings obtained from the GCN model are fed into the decoder, reconstruction loss of a predicted adjacency matrix is computed and then an optimization step is performed. You can also modify number of epochs, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fea440d59dba37d668d86c3ba21317b",
     "grade": false,
     "grade_id": "cell-ab892da4f2b04e92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "model = GCN(1433, 32, 16)\n",
    "decoder = DotProductDecoder(dropout_rate=0.1)\n",
    "rec_loss = ReconstructionLoss(dgl_cora)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    log.append(train_loss)\n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how unsupervised embeddings will work for the node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b877062593c1ae39ed18ce9b1fbc9fd7",
     "grade": true,
     "grade_id": "cell-08af8fff077a7121",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X = model.forward(adj, feat).data.numpy()\n",
    "y = label.numpy()\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X[train_mask], y[train_mask])\n",
    "\n",
    "y_true = y[test_mask]\n",
    "y_pred = lr.predict(X[test_mask])\n",
    "score = balanced_accuracy_score(y_true, y_pred)\n",
    "assert score >= 0.35\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Graph Attention Network (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply Graph Attention Network over an input signal.\n",
    "\n",
    "$$h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score bewteen node $i$ and node $j$:\n",
    "\n",
    "$$\\alpha_{ij}^{l} = \\text{Softmax}_{i} (e_{ij}^{l})$$\n",
    "\n",
    "$$e_{ij}^{l} = \\text{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "G = dgl.add_self_loop(G)\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GAT`. Use `GATConv` layers:\n",
    "`GATConv(1433, 8, num_heads=8)` $\\to$ `GATConv(8*8, 7, num_heads=1)`\n",
    "\n",
    "*Hint: use `F.elu` activation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdc8345eb40be185359aeb01fff87fde",
     "grade": false,
     "grade_id": "cell-2ac8340166410ded",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42f7e6eb09539ea2db5cca284c757afb",
     "grade": true,
     "grade_id": "cell-8b1838d547caf32d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GAT()\n",
    "assert str(model) == 'GAT(\\n  (conv1): GATConv(\\n    (fc): Linear(in_features=1433, out_features=64, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n  (conv2): GATConv(\\n    (fc): Linear(in_features=64, out_features=7, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), \n",
    "                       lr=0.005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f88b50e0154065046c516c78b67d0bb",
     "grade": false,
     "grade_id": "cell-8d019b9371d11f59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "877271c101d66ff526a820195815bb78",
     "grade": true,
     "grade_id": "cell-f0b5cbfec9b4510d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(G, features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. GraphSAGE model (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider GraphSAGE, a representation learning technique suitable for dynamic graphs. GraphSAGE is capable of predicting embedding of a new node, without requiring a re-training procedure. To do so, GraphSAGE learns aggregator functions that can induce the embedding of a new node given its features and neighborhood. This is called inductive learning.\n",
    "\n",
    "$$h_{\\mathcal{N}(i)}^{(l+1)} = \\mathrm{aggregate}\\left(\\{h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)$$\n",
    "$$h_{i}^{(l+1)} = \\sigma \\left(W \\cdot \\text{concat}(h_{i}^{l}, h_{\\mathcal{N}(i)}^{l+1}) \\right)$$\n",
    "$$h_{i}^{(l+1)} = \\mathrm{norm}(h_{i}^{l})$$\n",
    "\n",
    "Aggregator types here can be `mean`, `gcn`, `pool`, `lstm`. Consider GraphSAGE on the Karate Club graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G, with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us delete the node 31, train a model and then return it and predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(34)\n",
    "idx = idx[idx != 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[idx]\n",
    "features = features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G.subgraph(idx), with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let choose test and train nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [31, 32, 0, 11, 13, 2, 23, 29, 8]\n",
    "train_idx = list(set(np.arange(33)).difference(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the graph, test nodes are gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color = np.ones((33, 3))\n",
    "node_color[labels == 0] = plt.cm.tab10(0)[:3]\n",
    "node_color[labels == 1] = plt.cm.tab10(1)[:3]\n",
    "node_color[test_idx] = (0.9, 0.9, 0.9)\n",
    "\n",
    "nx.draw_kamada_kawai(G.subgraph(idx), with_labels=True, \n",
    "                     node_color=node_color, cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dgl graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graph = dgl.from_networkx(G.subgraph(idx))\n",
    "initial_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `SAGE`. Use `SAGEConv` layers with sizes $1 \\to 16 \\to 2$ and `mean` aggregation function. Put `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9399531298f5c0c2a65de6d6a0a15776",
     "grade": false,
     "grade_id": "cell-c83ca7ab5833bc2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, graph, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7324f456ebe940b2c3f71d704e741b82",
     "grade": true,
     "grade_id": "cell-a7513e546c544396",
     "locked": true,
     "points": 2.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE()\n",
    "assert str(model) == 'SAGE(\\n  (conv1): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=1, out_features=16, bias=False)\\n    (fc_neigh): Linear(in_features=1, out_features=16, bias=False)\\n  )\\n  (conv2): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=16, out_features=2, bias=False)\\n    (fc_neigh): Linear(in_features=16, out_features=2, bias=False)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    logits = model.forward(initial_graph, features)\n",
    "    loss = CrossEntropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(initial_graph, features)\n",
    "        test_loss = CrossEntropy(logits[test_idx], labels[test_idx])\n",
    "    \n",
    "    log.append([loss.item(), test_loss.item()])\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    \n",
    "    y_pred = torch.argmax(logits, 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    nx.draw_kamada_kawai(\n",
    "        G.subgraph(idx), with_labels=True, \n",
    "        node_color=['tab:orange' if i==1 else 'tab:blue' for i in y_pred], \n",
    "        cmap=plt.cm.tab10)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that prediction for the node 31 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dab650bfd86a91be05e721cd00e089e",
     "grade": true,
     "grade_id": "cell-41e44fe2c4bf18a1",
     "locked": true,
     "points": 2.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "graph = dgl.from_networkx(G)\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])\n",
    "predictions = torch.argmax(model(graph, features), 1)\n",
    "assert predictions[31] == labels[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
