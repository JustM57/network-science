{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import link_prediction as lp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dgl.nn.pytorch.conv import SAGEConv, GraphConv, DotGatConv\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def negative_sampling_etc(H, un_edges, k=1):\n",
    "    negative_edges = set()\n",
    "    all_edges = list(H.edges)\n",
    "    nodes = list(H.nodes)\n",
    "    for i in range(k):\n",
    "        for u, _ in all_edges:\n",
    "            exists = True\n",
    "            while exists:\n",
    "                v = np.random.choice(nodes, size=1)[0]\n",
    "                if (u != v) and ((u, v) not in all_edges):\n",
    "                    if (u, v) not in negative_edges:\n",
    "                        if (u, v) not in un_edges:\n",
    "                            negative_edges.add((u, v))\n",
    "                            exists = False\n",
    "    return list(negative_edges)\n",
    "\n",
    "\n",
    "def negative_sampling(H, un_edges):\n",
    "    negative_edges = set()\n",
    "    all_edges = list(H.edges)\n",
    "    nodes = list(H.nodes)\n",
    "    for u, _ in all_edges:\n",
    "        exists = True\n",
    "        while exists:\n",
    "            v = np.random.choice(nodes, size=1)[0]\n",
    "            if (u != v) and ((u, v) not in all_edges):\n",
    "                if (u, v) not in negative_edges:\n",
    "                    negative_edges.add((u, v))\n",
    "                    exists = False\n",
    "    return list(negative_edges)\n",
    "\n",
    "\n",
    "def negative_sampling_rand(H, un_edges, k=1):\n",
    "    negative_edges = set()\n",
    "    all_edges = list(H.edges)\n",
    "    nodes = list(H.nodes)\n",
    "    while len(negative_edges) < k * len(all_edges):\n",
    "        u, v = np.random.choice(nodes, size=2)\n",
    "        if (u != v) and ((u, v) not in all_edges):\n",
    "            if (u, v) not in un_edges:\n",
    "                negative_edges.add((u, v))\n",
    "    return list(negative_edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, conv1, conv2):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.conv2_type = conv2\n",
    "        self.conv3_type = conv2\n",
    "        if conv1 == 'sage':\n",
    "            self.conv1 = SAGEConv(in_feats, h_feats, 'gcn')\n",
    "        else:\n",
    "            self.conv1 = GraphConv(in_feats, h_feats, weight=True, norm='both')\n",
    "        if conv2 == 'sage':\n",
    "            self.conv2 = SAGEConv(h_feats, h_feats, 'gcn')\n",
    "        elif conv2 == 'gat':\n",
    "            self.conv2 = DotGatConv(h_feats, h_feats, num_heads=1)\n",
    "        elif conv2 == 'gcn':\n",
    "            self.conv2 = GraphConv(h_feats, h_feats, weight=True, norm='both')\n",
    "        # if conv3 == 'sage':\n",
    "        #     self.conv3 = SAGEConv(h_feats, h_feats, 'gcn')\n",
    "        # elif conv3 == 'gat':\n",
    "        #     self.conv3 = DotGatConv(h_feats, h_feats, num_heads=1)\n",
    "        # elif conv3 == 'gcn':\n",
    "        #     self.conv3 = GraphConv(h_feats, h_feats, weight=True, norm='both')\n",
    "\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        if self.conv2_type != 'not':\n",
    "            h = F.relu(h)\n",
    "            h = self.conv2(g, h)\n",
    "            if self.conv2_type == 'gat':\n",
    "                h = torch.flatten(h, start_dim=1)\n",
    "            # if self.conv3_type != 'not':\n",
    "            #     h = F.relu(h)\n",
    "            #     h = self.conv3(g, h)\n",
    "            #     if self.conv3_type == 'gat':\n",
    "            #         h = torch.flatten(h, start_dim=1)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super().__init__()\n",
    "        # self.mapping = mapping\n",
    "        self.W1 = nn.Linear(in_feats, h_feats)\n",
    "        self.P1 = nn.Linear(in_feats, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = edges.src['h'] * edges.dst['h']\n",
    "        w = F.relu(self.W1(h))\n",
    "        p = torch.sigmoid(self.P1(h))\n",
    "        h = self.W2(w * p)\n",
    "        return {'score': h.squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def train_sigle_model(sample_func):\n",
    "    def objective(trial):\n",
    "        conv1 = trial.suggest_categorical(\"conv1\", [\"sage\", \"gcn\"])\n",
    "        conv2 = trial.suggest_categorical(\"conv2\", [\"sage\", \"gat\", \"gcn\", \"not\"])\n",
    "        # conv3 = trial.suggest_categorical(\"conv3\", [\"sage\", \"gat\", \"gcn\", \"not\"])\n",
    "        # mapping = trial.suggest_categorical('mapping', ['dot', 'sum', 'flat'])\n",
    "        encoder_hidden = trial.suggest_int(\"encoder_hidden\", 32, 1024, log=True)\n",
    "        predictor_hidden = trial.suggest_int(\"predictor_hidden\", 8, 128, log=True)\n",
    "        encoder = GraphEncoder(32, encoder_hidden, conv1, conv2)\n",
    "        predictor = Predictor(encoder_hidden, predictor_hidden)\n",
    "        optimizer = torch.optim.Adam(itertools.chain(encoder.parameters(), predictor.parameters()), lr=1e-3)\n",
    "        scores = {}\n",
    "        for epoch in range(500):\n",
    "            h = encoder(train_g , torch.tensor(node_emb, dtype=torch.float))\n",
    "            pos_score = predictor(train_pos_g, h)\n",
    "            neg_score = predictor(train_neg_g, h)\n",
    "            loss = compute_loss(pos_score, neg_score)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 9:\n",
    "                with torch.no_grad():\n",
    "                    pos_score = torch.sigmoid(predictor(test_pos_g, h))\n",
    "                    neg_score = torch.sigmoid(predictor(test_neg_g, h))\n",
    "                scores[epoch] = compute_auc(pos_score, neg_score)\n",
    "                trial.report(scores[epoch], epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "        best_epoch = max(scores.items(), key=lambda x: x[1])\n",
    "        trial.set_user_attr(\"epochs\", best_epoch[0])\n",
    "        return best_epoch[1]\n",
    "\n",
    "\n",
    "    G, node_emb = lp.load_initial_graph()\n",
    "    positive_edges = list(G.edges())\n",
    "    un_edges = lp.get_unlabeled_edges()\n",
    "    negative_edges = sample_func(G, un_edges, k=1)\n",
    "    train_pos, test_pos = train_test_split(positive_edges, test_size=0.1)\n",
    "    train_neg, test_neg = train_test_split(negative_edges, test_size=0.1)\n",
    "    train_pos_g = dgl.graph(train_pos, num_nodes=len(G))\n",
    "    train_neg_g = dgl.graph(train_neg, num_nodes=len(G))\n",
    "    test_pos_g = dgl.graph(test_pos, num_nodes=len(G))\n",
    "    test_neg_g = dgl.graph(test_neg, num_nodes=len(G))\n",
    "    un_g = dgl.graph(un_edges, num_nodes=len(G))\n",
    "\n",
    "    trainG = nx.DiGraph()\n",
    "    trainG.add_nodes_from(list(range(node_emb.shape[0])))\n",
    "    trainG.add_edges_from(train_pos)\n",
    "    train_g = dgl.from_networkx(trainG).add_self_loop()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    trial = study.best_trial\n",
    "\n",
    "    train_g = dgl.from_networkx(G).add_self_loop()\n",
    "    train_pos_g = dgl.graph(positive_edges, num_nodes=len(G))\n",
    "    train_neg_g = dgl.graph(negative_edges, num_nodes=len(G))\n",
    "    encoder = GraphEncoder(32, trial.params['encoder_hidden'], trial.params['conv1'], trial.params['conv2'])\n",
    "    predictor = Predictor(trial.params['encoder_hidden'], trial.params['predictor_hidden'])\n",
    "    optimizer = torch.optim.Adam(itertools.chain(encoder.parameters(), predictor.parameters()), lr=1e-3)\n",
    "    for epoch in range(trial.user_attrs[\"epochs\"]):\n",
    "        h = encoder(train_g , torch.tensor(node_emb, dtype=torch.float))\n",
    "        pos_score = predictor(train_pos_g, h)\n",
    "        neg_score = predictor(train_neg_g, h)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        proba = torch.sigmoid(predictor(un_g, h)).detach().numpy()\n",
    "    return proba"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 18:00:05,586]\u001B[0m A new study created in memory with name: no-name-5439c5e9-e2a2-414a-94d5-fb3128040d7a\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:00:22,477]\u001B[0m Trial 0 finished with value: 0.836979891297202 and parameters: {'conv1': 'gcn', 'conv2': 'sage', 'encoder_hidden': 83, 'predictor_hidden': 120}. Best is trial 0 with value: 0.836979891297202.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:00:46,840]\u001B[0m Trial 1 finished with value: 0.8479005731221351 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 202, 'predictor_hidden': 9}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:01:52,052]\u001B[0m Trial 2 finished with value: 0.8418423473415244 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 549, 'predictor_hidden': 66}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:02:04,603]\u001B[0m Trial 3 finished with value: 0.8269623065913672 and parameters: {'conv1': 'gcn', 'conv2': 'gcn', 'encoder_hidden': 56, 'predictor_hidden': 101}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:06:08,096]\u001B[0m Trial 4 finished with value: 0.8339472478303998 and parameters: {'conv1': 'gcn', 'conv2': 'gcn', 'encoder_hidden': 981, 'predictor_hidden': 14}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:06:11,067]\u001B[0m Trial 5 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:06:11,206]\u001B[0m Trial 6 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:06:11,457]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:21,387]\u001B[0m Trial 8 finished with value: 0.842493434345156 and parameters: {'conv1': 'gcn', 'conv2': 'not', 'encoder_hidden': 529, 'predictor_hidden': 120}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:24,171]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:25,889]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:26,730]\u001B[0m Trial 11 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:39,408]\u001B[0m Trial 12 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:39,636]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:42,974]\u001B[0m Trial 14 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:07:47,042]\u001B[0m Trial 15 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:08:10,804]\u001B[0m Trial 16 finished with value: 0.8446692434193706 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 179, 'predictor_hidden': 51}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:08:21,441]\u001B[0m Trial 17 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:08:31,633]\u001B[0m Trial 18 finished with value: 0.8453924028811451 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 96, 'predictor_hidden': 25}. Best is trial 1 with value: 0.8479005731221351.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:08:31,840]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:08:33,015]\u001B[0m Trial 20 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:03,680]\u001B[0m Trial 21 finished with value: 0.8497966144449765 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 227, 'predictor_hidden': 30}. Best is trial 21 with value: 0.8497966144449765.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:04,453]\u001B[0m Trial 22 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:14,720]\u001B[0m Trial 23 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:14,996]\u001B[0m Trial 24 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:15,197]\u001B[0m Trial 25 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:09:15,314]\u001B[0m Trial 26 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:09,065]\u001B[0m Trial 27 finished with value: 0.8501056007604617 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 354, 'predictor_hidden': 20}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:10,345]\u001B[0m Trial 28 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:13,655]\u001B[0m Trial 29 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:14,302]\u001B[0m Trial 30 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:15,527]\u001B[0m Trial 31 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:18,679]\u001B[0m Trial 32 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:19,250]\u001B[0m Trial 33 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:22,289]\u001B[0m Trial 34 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:22,568]\u001B[0m Trial 35 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:23,450]\u001B[0m Trial 36 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:23,709]\u001B[0m Trial 37 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:24,027]\u001B[0m Trial 38 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:24,175]\u001B[0m Trial 39 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:25,559]\u001B[0m Trial 40 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:26,047]\u001B[0m Trial 41 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:26,650]\u001B[0m Trial 42 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:27,411]\u001B[0m Trial 43 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:32,181]\u001B[0m Trial 44 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:32,729]\u001B[0m Trial 45 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:33,834]\u001B[0m Trial 46 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:34,305]\u001B[0m Trial 47 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:35,066]\u001B[0m Trial 48 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:39,991]\u001B[0m Trial 49 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:42,277]\u001B[0m Trial 50 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:43,799]\u001B[0m Trial 51 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:44,697]\u001B[0m Trial 52 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:45,510]\u001B[0m Trial 53 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:46,271]\u001B[0m Trial 54 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:48,301]\u001B[0m Trial 55 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:48,570]\u001B[0m Trial 56 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:51,668]\u001B[0m Trial 57 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:51,855]\u001B[0m Trial 58 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:10:53,081]\u001B[0m Trial 59 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:11:00,517]\u001B[0m Trial 60 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:12:55,784]\u001B[0m Trial 61 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:13:50,770]\u001B[0m Trial 62 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:14:50,998]\u001B[0m Trial 63 finished with value: 0.8436823864164843 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 516, 'predictor_hidden': 73}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:15:53,312]\u001B[0m Trial 64 finished with value: 0.8441050816439728 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 484, 'predictor_hidden': 120}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:16:54,506]\u001B[0m Trial 65 finished with value: 0.8438279922609764 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 487, 'predictor_hidden': 87}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:18,074]\u001B[0m Trial 66 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:19,132]\u001B[0m Trial 67 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:19,310]\u001B[0m Trial 68 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:24,289]\u001B[0m Trial 69 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:32,753]\u001B[0m Trial 70 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:17:41,276]\u001B[0m Trial 71 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:19:32,273]\u001B[0m Trial 72 finished with value: 0.8445686828612182 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 849, 'predictor_hidden': 71}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:21:21,380]\u001B[0m Trial 73 finished with value: 0.8438377317823471 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 801, 'predictor_hidden': 91}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:26,227]\u001B[0m Trial 74 finished with value: 0.8444267293372403 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 851, 'predictor_hidden': 112}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:28,467]\u001B[0m Trial 75 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:28,810]\u001B[0m Trial 76 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:33,611]\u001B[0m Trial 77 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:36,076]\u001B[0m Trial 78 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:45,862]\u001B[0m Trial 79 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:23:51,646]\u001B[0m Trial 80 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:24:54,571]\u001B[0m Trial 81 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:26:21,729]\u001B[0m Trial 82 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:49,537]\u001B[0m Trial 83 finished with value: 0.8467895372217722 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 1019, 'predictor_hidden': 101}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:50,074]\u001B[0m Trial 84 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:54,872]\u001B[0m Trial 85 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:55,787]\u001B[0m Trial 86 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:56,173]\u001B[0m Trial 87 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:28:58,443]\u001B[0m Trial 88 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:29:01,405]\u001B[0m Trial 89 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:29:01,855]\u001B[0m Trial 90 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:30:38,989]\u001B[0m Trial 91 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:30:42,048]\u001B[0m Trial 92 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:32:16,069]\u001B[0m Trial 93 finished with value: 0.8441803194465615 and parameters: {'conv1': 'sage', 'conv2': 'not', 'encoder_hidden': 686, 'predictor_hidden': 94}. Best is trial 27 with value: 0.8501056007604617.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:32:18,265]\u001B[0m Trial 94 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:32:55,697]\u001B[0m Trial 95 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:32:56,222]\u001B[0m Trial 96 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:32:57,528]\u001B[0m Trial 97 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:33:09,439]\u001B[0m Trial 98 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:33:10,027]\u001B[0m Trial 99 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:34:34,373]\u001B[0m A new study created in memory with name: no-name-283a050e-14f3-4abd-b817-b88ce119f29c\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:35:00,687]\u001B[0m Trial 0 finished with value: 0.8540152881266956 and parameters: {'conv1': 'gcn', 'conv2': 'gcn', 'encoder_hidden': 210, 'predictor_hidden': 26}. Best is trial 0 with value: 0.8540152881266956.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:36:11,203]\u001B[0m Trial 1 finished with value: 0.8567905647412769 and parameters: {'conv1': 'gcn', 'conv2': 'gcn', 'encoder_hidden': 434, 'predictor_hidden': 19}. Best is trial 1 with value: 0.8567905647412769.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:40:08,504]\u001B[0m Trial 2 finished with value: 0.8702754190550813 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 999, 'predictor_hidden': 23}. Best is trial 2 with value: 0.8702754190550813.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:40:44,357]\u001B[0m Trial 3 finished with value: 0.872741465866143 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 266, 'predictor_hidden': 12}. Best is trial 3 with value: 0.872741465866143.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:15,232]\u001B[0m Trial 4 finished with value: 0.8548918450500586 and parameters: {'conv1': 'gcn', 'conv2': 'sage', 'encoder_hidden': 213, 'predictor_hidden': 47}. Best is trial 3 with value: 0.872741465866143.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:17,290]\u001B[0m Trial 5 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:17,957]\u001B[0m Trial 6 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:22,060]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:26,719]\u001B[0m Trial 8 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:26,925]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:27,004]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:31,713]\u001B[0m Trial 11 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:46,101]\u001B[0m Trial 12 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:47,217]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:47,587]\u001B[0m Trial 14 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:48,156]\u001B[0m Trial 15 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:41:48,287]\u001B[0m Trial 16 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:42:34,387]\u001B[0m Trial 17 finished with value: 0.8703762231012682 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 311, 'predictor_hidden': 38}. Best is trial 3 with value: 0.872741465866143.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:43:21,733]\u001B[0m Trial 18 finished with value: 0.871679371060668 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 315, 'predictor_hidden': 39}. Best is trial 3 with value: 0.872741465866143.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:43:29,832]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:43:30,654]\u001B[0m Trial 20 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:15,019]\u001B[0m Trial 21 finished with value: 0.8718561433735462 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 301, 'predictor_hidden': 34}. Best is trial 3 with value: 0.872741465866143.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:17,345]\u001B[0m Trial 22 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:18,660]\u001B[0m Trial 23 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:35,054]\u001B[0m Trial 24 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:35,475]\u001B[0m Trial 25 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:36,863]\u001B[0m Trial 26 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:38,167]\u001B[0m Trial 27 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:41,039]\u001B[0m Trial 28 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:44:43,090]\u001B[0m Trial 29 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:47:24,376]\u001B[0m Trial 30 finished with value: 0.8767110512888066 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 754, 'predictor_hidden': 29}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:49:27,572]\u001B[0m Trial 31 finished with value: 0.8716521004008301 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 635, 'predictor_hidden': 27}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:49:30,915]\u001B[0m Trial 32 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:08,363]\u001B[0m Trial 33 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:10,401]\u001B[0m Trial 34 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:12,145]\u001B[0m Trial 35 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:16,065]\u001B[0m Trial 36 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:17,197]\u001B[0m Trial 37 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:20,417]\u001B[0m Trial 38 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:53,085]\u001B[0m Trial 39 finished with value: 0.8673866770165313 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 181, 'predictor_hidden': 126}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:55,038]\u001B[0m Trial 40 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:50:57,986]\u001B[0m Trial 41 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:52:45,473]\u001B[0m Trial 42 finished with value: 0.8721955656933151 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 568, 'predictor_hidden': 31}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:52:48,371]\u001B[0m Trial 43 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:52:52,565]\u001B[0m Trial 44 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:11,082]\u001B[0m Trial 45 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:12,531]\u001B[0m Trial 46 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:15,795]\u001B[0m Trial 47 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:34,453]\u001B[0m Trial 48 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:39,945]\u001B[0m Trial 49 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:41,215]\u001B[0m Trial 50 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:44,218]\u001B[0m Trial 51 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:53:49,306]\u001B[0m Trial 52 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:56:11,421]\u001B[0m Trial 53 finished with value: 0.8741877847896921 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 666, 'predictor_hidden': 37}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:00,635]\u001B[0m Trial 54 finished with value: 0.8710453282194353 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 555, 'predictor_hidden': 37}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:01,221]\u001B[0m Trial 55 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:02,883]\u001B[0m Trial 56 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:03,032]\u001B[0m Trial 57 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:06,271]\u001B[0m Trial 58 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:27,506]\u001B[0m Trial 59 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:28,692]\u001B[0m Trial 60 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:30,374]\u001B[0m Trial 61 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:33,214]\u001B[0m Trial 62 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 18:58:35,675]\u001B[0m Trial 63 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:26,604]\u001B[0m Trial 64 finished with value: 0.8718902316983437 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 574, 'predictor_hidden': 33}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:30,044]\u001B[0m Trial 65 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:32,125]\u001B[0m Trial 66 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:38,385]\u001B[0m Trial 67 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:41,226]\u001B[0m Trial 68 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:41,640]\u001B[0m Trial 69 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:52,335]\u001B[0m Trial 70 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:00:54,690]\u001B[0m Trial 71 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:01:00,039]\u001B[0m Trial 72 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:01:03,441]\u001B[0m Trial 73 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:01:07,346]\u001B[0m Trial 74 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:39,422]\u001B[0m Trial 75 finished with value: 0.875624851167939 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 486, 'predictor_hidden': 36}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:41,382]\u001B[0m Trial 76 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:42,079]\u001B[0m Trial 77 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:42,663]\u001B[0m Trial 78 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:46,391]\u001B[0m Trial 79 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:47,465]\u001B[0m Trial 80 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:02:49,939]\u001B[0m Trial 81 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:04:58,983]\u001B[0m Trial 82 finished with value: 0.8764035259015266 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 605, 'predictor_hidden': 31}. Best is trial 30 with value: 0.8767110512888066.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:00,753]\u001B[0m Trial 83 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:03,496]\u001B[0m Trial 84 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:19,076]\u001B[0m Trial 85 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:21,527]\u001B[0m Trial 86 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:23,695]\u001B[0m Trial 87 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:25,206]\u001B[0m Trial 88 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:28,206]\u001B[0m Trial 89 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:31,583]\u001B[0m Trial 90 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:35,228]\u001B[0m Trial 91 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:38,261]\u001B[0m Trial 92 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:40,291]\u001B[0m Trial 93 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:42,253]\u001B[0m Trial 94 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:46,679]\u001B[0m Trial 95 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:47,878]\u001B[0m Trial 96 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:50,736]\u001B[0m Trial 97 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:52,423]\u001B[0m Trial 98 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:05:54,387]\u001B[0m Trial 99 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:08:27,815]\u001B[0m A new study created in memory with name: no-name-b73918be-74df-4a52-bf9e-269e38e445b5\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:10:14,599]\u001B[0m Trial 0 finished with value: 0.853097338237507 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 592, 'predictor_hidden': 8}. Best is trial 0 with value: 0.853097338237507.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:10:31,737]\u001B[0m Trial 1 finished with value: 0.8430373866137096 and parameters: {'conv1': 'sage', 'conv2': 'gat', 'encoder_hidden': 91, 'predictor_hidden': 92}. Best is trial 0 with value: 0.853097338237507.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:10:40,597]\u001B[0m Trial 2 finished with value: 0.8172106108189524 and parameters: {'conv1': 'gcn', 'conv2': 'gcn', 'encoder_hidden': 41, 'predictor_hidden': 30}. Best is trial 0 with value: 0.853097338237507.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:11:57,363]\u001B[0m Trial 3 finished with value: 0.8510033411428062 and parameters: {'conv1': 'gcn', 'conv2': 'gat', 'encoder_hidden': 394, 'predictor_hidden': 60}. Best is trial 0 with value: 0.853097338237507.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:15:28,654]\u001B[0m Trial 4 finished with value: 0.8563961141257636 and parameters: {'conv1': 'gcn', 'conv2': 'sage', 'encoder_hidden': 813, 'predictor_hidden': 47}. Best is trial 4 with value: 0.8563961141257636.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:16:31,960]\u001B[0m Trial 5 finished with value: 0.8590033839967004 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 359, 'predictor_hidden': 12}. Best is trial 5 with value: 0.8590033839967004.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:17:10,300]\u001B[0m Trial 6 finished with value: 0.857475009605603 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 194, 'predictor_hidden': 128}. Best is trial 5 with value: 0.8590033839967004.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:17:15,443]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:17:26,532]\u001B[0m Trial 8 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:17:27,701]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:17:32,756]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:18:01,499]\u001B[0m Trial 11 finished with value: 0.8585446525401402 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 212, 'predictor_hidden': 17}. Best is trial 5 with value: 0.8590033839967004.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:18:35,397]\u001B[0m Trial 12 finished with value: 0.8560902931547236 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 233, 'predictor_hidden': 16}. Best is trial 5 with value: 0.8590033839967004.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:18:41,085]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:22:55,930]\u001B[0m Trial 14 finished with value: 0.8682778432219506 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 982, 'predictor_hidden': 20}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:01,119]\u001B[0m Trial 15 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:03,629]\u001B[0m Trial 16 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:09,043]\u001B[0m Trial 17 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:11,284]\u001B[0m Trial 18 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:12,765]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:13,707]\u001B[0m Trial 20 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:15,375]\u001B[0m Trial 21 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:23:19,190]\u001B[0m Trial 22 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:24:52,474]\u001B[0m Trial 23 finished with value: 0.856222750645365 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 465, 'predictor_hidden': 11}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:24:56,161]\u001B[0m Trial 24 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:24:56,332]\u001B[0m Trial 25 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:00,138]\u001B[0m Trial 26 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:00,458]\u001B[0m Trial 27 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:00,600]\u001B[0m Trial 28 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:01,128]\u001B[0m Trial 29 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:02,194]\u001B[0m Trial 30 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:02,701]\u001B[0m Trial 31 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:25:05,265]\u001B[0m Trial 32 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:26:00,367]\u001B[0m Trial 33 finished with value: 0.8632868254955346 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 293, 'predictor_hidden': 83}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:26:55,152]\u001B[0m Trial 34 finished with value: 0.860714130925464 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 298, 'predictor_hidden': 75}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:26:56,996]\u001B[0m Trial 35 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:26:58,353]\u001B[0m Trial 36 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:01,886]\u001B[0m Trial 37 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:02,869]\u001B[0m Trial 38 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:04,809]\u001B[0m Trial 39 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:06,909]\u001B[0m Trial 40 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:07,743]\u001B[0m Trial 41 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:09,179]\u001B[0m Trial 42 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:13,294]\u001B[0m Trial 43 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:13,691]\u001B[0m Trial 44 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:15,615]\u001B[0m Trial 45 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:18,989]\u001B[0m Trial 46 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:23,975]\u001B[0m Trial 47 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:25,122]\u001B[0m Trial 48 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:28,330]\u001B[0m Trial 49 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:28,773]\u001B[0m Trial 50 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:29,576]\u001B[0m Trial 51 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:30,007]\u001B[0m Trial 52 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:30,976]\u001B[0m Trial 53 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:32,393]\u001B[0m Trial 54 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:32,834]\u001B[0m Trial 55 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:33,720]\u001B[0m Trial 56 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:34,077]\u001B[0m Trial 57 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:45,347]\u001B[0m Trial 58 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:45,893]\u001B[0m Trial 59 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:27:48,558]\u001B[0m Trial 60 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:24,899]\u001B[0m Trial 61 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:37,570]\u001B[0m Trial 62 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:38,369]\u001B[0m Trial 63 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:45,826]\u001B[0m Trial 64 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:48,721]\u001B[0m Trial 65 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:28:59,942]\u001B[0m Trial 66 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:29:05,420]\u001B[0m Trial 67 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:29:26,751]\u001B[0m Trial 68 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:29:27,237]\u001B[0m Trial 69 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:29:31,255]\u001B[0m Trial 70 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:29:32,918]\u001B[0m Trial 71 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:05,117]\u001B[0m Trial 72 finished with value: 0.8626695833286665 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 504, 'predictor_hidden': 10}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:19,296]\u001B[0m Trial 73 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:20,296]\u001B[0m Trial 74 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:21,541]\u001B[0m Trial 75 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:24,115]\u001B[0m Trial 76 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:34,084]\u001B[0m Trial 77 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:31:34,213]\u001B[0m Trial 78 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:33:27,688]\u001B[0m Trial 79 finished with value: 0.8656257715527085 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 498, 'predictor_hidden': 110}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:35:22,953]\u001B[0m Trial 80 finished with value: 0.8639398604034403 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 495, 'predictor_hidden': 110}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:35:24,711]\u001B[0m Trial 81 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:35:27,240]\u001B[0m Trial 82 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:37:16,553]\u001B[0m Trial 83 finished with value: 0.8591740691087218 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 484, 'predictor_hidden': 123}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:37:19,025]\u001B[0m Trial 84 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:37:21,597]\u001B[0m Trial 85 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:38:41,383]\u001B[0m Trial 86 finished with value: 0.8598904109055369 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 396, 'predictor_hidden': 86}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:40:11,504]\u001B[0m Trial 87 finished with value: 0.86581812709978 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 437, 'predictor_hidden': 84}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:40:13,254]\u001B[0m Trial 88 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:40:14,791]\u001B[0m Trial 89 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:43:12,669]\u001B[0m Trial 90 finished with value: 0.8649917287114759 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 701, 'predictor_hidden': 124}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:46:09,012]\u001B[0m Trial 91 finished with value: 0.8627876750252863 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 701, 'predictor_hidden': 123}. Best is trial 14 with value: 0.8682778432219506.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:46:11,403]\u001B[0m Trial 92 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:46:14,348]\u001B[0m Trial 93 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:46:18,191]\u001B[0m Trial 94 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:46:20,317]\u001B[0m Trial 95 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:49:09,083]\u001B[0m Trial 96 finished with value: 0.8687419314152645 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 731, 'predictor_hidden': 76}. Best is trial 96 with value: 0.8687419314152645.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:49:12,702]\u001B[0m Trial 97 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:49:16,295]\u001B[0m Trial 98 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:49:21,903]\u001B[0m Trial 99 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:52:34,175]\u001B[0m A new study created in memory with name: no-name-a74f990c-c447-4ae6-afa8-e37fb30e479f\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:54:05,609]\u001B[0m Trial 0 finished with value: 0.8725447275344549 and parameters: {'conv1': 'sage', 'conv2': 'gat', 'encoder_hidden': 490, 'predictor_hidden': 30}. Best is trial 0 with value: 0.8725447275344549.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:55:08,300]\u001B[0m Trial 1 finished with value: 0.8710545807647374 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 369, 'predictor_hidden': 12}. Best is trial 0 with value: 0.8725447275344549.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:56:28,620]\u001B[0m Trial 2 finished with value: 0.8575726483073441 and parameters: {'conv1': 'gcn', 'conv2': 'sage', 'encoder_hidden': 437, 'predictor_hidden': 20}. Best is trial 0 with value: 0.8725447275344549.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:05,484]\u001B[0m Trial 3 finished with value: 0.8638685184093999 and parameters: {'conv1': 'gcn', 'conv2': 'not', 'encoder_hidden': 715, 'predictor_hidden': 27}. Best is trial 0 with value: 0.8725447275344549.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:13,605]\u001B[0m Trial 4 finished with value: 0.8446244416210654 and parameters: {'conv1': 'sage', 'conv2': 'sage', 'encoder_hidden': 42, 'predictor_hidden': 24}. Best is trial 0 with value: 0.8725447275344549.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:17,317]\u001B[0m Trial 5 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:23,487]\u001B[0m Trial 6 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:29,229]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:29,363]\u001B[0m Trial 8 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:30,017]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:31,390]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:58:33,377]\u001B[0m Trial 11 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:59:06,661]\u001B[0m Trial 12 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:59:52,975]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 19:59:53,458]\u001B[0m Trial 14 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:01:13,471]\u001B[0m Trial 15 finished with value: 0.87503609710108 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 433, 'predictor_hidden': 54}. Best is trial 15 with value: 0.87503609710108.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:01:18,758]\u001B[0m Trial 16 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:01:21,798]\u001B[0m Trial 17 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:05:18,329]\u001B[0m Trial 18 finished with value: 0.8758522689919449 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 977, 'predictor_hidden': 36}. Best is trial 18 with value: 0.8758522689919449.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:05:23,177]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:09:29,368]\u001B[0m Trial 20 finished with value: 0.8764529539724828 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 973, 'predictor_hidden': 75}. Best is trial 20 with value: 0.8764529539724828.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:42,262]\u001B[0m Trial 21 finished with value: 0.8790234571502452 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 967, 'predictor_hidden': 80}. Best is trial 21 with value: 0.8790234571502452.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:47,590]\u001B[0m Trial 22 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:51,150]\u001B[0m Trial 23 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:51,349]\u001B[0m Trial 24 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:57,284]\u001B[0m Trial 25 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:13:58,773]\u001B[0m Trial 26 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:02,854]\u001B[0m Trial 27 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:05,548]\u001B[0m Trial 28 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:07,547]\u001B[0m Trial 29 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:08,635]\u001B[0m Trial 30 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:12,728]\u001B[0m Trial 31 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:14:14,252]\u001B[0m Trial 32 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:15:02,261]\u001B[0m Trial 33 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:15:03,494]\u001B[0m Trial 34 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:00,443]\u001B[0m Trial 35 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:03,171]\u001B[0m Trial 36 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:06,941]\u001B[0m Trial 37 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:07,886]\u001B[0m Trial 38 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:11,443]\u001B[0m Trial 39 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:16,913]\u001B[0m Trial 40 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:18,197]\u001B[0m Trial 41 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:20,767]\u001B[0m Trial 42 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:21,713]\u001B[0m Trial 43 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:22,116]\u001B[0m Trial 44 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:22,367]\u001B[0m Trial 45 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:26,388]\u001B[0m Trial 46 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:28,158]\u001B[0m Trial 47 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:28,853]\u001B[0m Trial 48 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:32,056]\u001B[0m Trial 49 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:34,782]\u001B[0m Trial 50 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:36,036]\u001B[0m Trial 51 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:37,088]\u001B[0m Trial 52 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:39,223]\u001B[0m Trial 53 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:39,880]\u001B[0m Trial 54 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:40,987]\u001B[0m Trial 55 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:42,644]\u001B[0m Trial 56 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:44,913]\u001B[0m Trial 57 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:45,691]\u001B[0m Trial 58 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:50,639]\u001B[0m Trial 59 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:53,089]\u001B[0m Trial 60 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:55,014]\u001B[0m Trial 61 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:16:56,644]\u001B[0m Trial 62 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:08,437]\u001B[0m Trial 63 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:09,485]\u001B[0m Trial 64 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:11,572]\u001B[0m Trial 65 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:13,054]\u001B[0m Trial 66 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:17,056]\u001B[0m Trial 67 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:19,837]\u001B[0m Trial 68 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:17:33,057]\u001B[0m Trial 69 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:35,055]\u001B[0m Trial 70 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:41,923]\u001B[0m Trial 71 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:43,356]\u001B[0m Trial 72 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:44,466]\u001B[0m Trial 73 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:51,864]\u001B[0m Trial 74 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:52,838]\u001B[0m Trial 75 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:55,527]\u001B[0m Trial 76 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:18:59,378]\u001B[0m Trial 77 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:19:02,632]\u001B[0m Trial 78 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:19:07,240]\u001B[0m Trial 79 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:20:48,268]\u001B[0m Trial 80 finished with value: 0.8735819865604346 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 529, 'predictor_hidden': 26}. Best is trial 21 with value: 0.8790234571502452.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:21:01,464]\u001B[0m Trial 81 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:42,047]\u001B[0m Trial 82 finished with value: 0.8725208657070966 and parameters: {'conv1': 'sage', 'conv2': 'gcn', 'encoder_hidden': 526, 'predictor_hidden': 35}. Best is trial 21 with value: 0.8790234571502452.\u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:44,094]\u001B[0m Trial 83 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:46,539]\u001B[0m Trial 84 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:50,062]\u001B[0m Trial 85 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:51,722]\u001B[0m Trial 86 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:55,835]\u001B[0m Trial 87 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:57,887]\u001B[0m Trial 88 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:22:59,177]\u001B[0m Trial 89 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:03,786]\u001B[0m Trial 90 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:05,703]\u001B[0m Trial 91 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:08,240]\u001B[0m Trial 92 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:09,266]\u001B[0m Trial 93 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:12,041]\u001B[0m Trial 94 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:13,150]\u001B[0m Trial 95 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:14,732]\u001B[0m Trial 96 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:16,874]\u001B[0m Trial 97 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:20,666]\u001B[0m Trial 98 pruned. \u001B[0m\n",
      "\u001B[32m[I 2022-04-03 20:23:20,811]\u001B[0m Trial 99 pruned. \u001B[0m\n"
     ]
    }
   ],
   "source": [
    "proba_list = [train_sigle_model(sample_func) for sample_func in [\n",
    "    negative_sampling_rand, negative_sampling_rand, negative_sampling_etc, negative_sampling_etc]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "forecast = pd.DataFrame(proba_list).mean(axis=0)\n",
    "median = forecast.median()\n",
    "forecast.map(lambda x: x >= median).astype(int).to_csv('mean_50.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.31999665"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast = pd.DataFrame(proba_list).max(axis=0)\n",
    "median = forecast.median()\n",
    "forecast.map(lambda x: x >= median).astype(int).to_csv('max_50.txt', index=False, header=False)\n",
    "median"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "forecast.map(lambda x: x >= 2).astype(int).to_csv('zero.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n44009    0\n44010    0\n44011    0\n44012    0\n44013    0\nLength: 44014, dtype: int64"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.map(lambda x: x >= 2).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(proba_list).mean(axis=0).round().astype(int).to_csv('final_pred_mean_sigmoid.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).mean(axis=0).map(lambda x: x > 0.4).astype(int).to_csv('final_pred_mean_sigmoid_04.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).mean(axis=0).map(lambda x: x > 0.14185).astype(int).to_csv('final_pred_mean_sigmoid_50.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).max(axis=0).map(lambda x: x > 0.5).astype(int).to_csv('final_pred_mean_sigmoid_max.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).max(axis=0).map(lambda x: x > 0.4).astype(int).to_csv('final_pred_mean_sigmoid_max_04.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).max(axis=0).map(lambda x: x > 0.32079).astype(int).to_csv('final_pred_mean_sigmoid_max_50.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list[:2]).mean(axis=0).round().astype(int).to_csv('final_pred_mean_sigmoid_v1.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list[2:]).mean(axis=0).round().astype(int).to_csv('final_pred_mean_sigmoid_v2.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for idx, preds in enumerate(proba_list):\n",
    "    pd.Series(np.round(preds)).astype(int).to_csv(f'final_pred_{idx}.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_list).mean(axis=0).round().astype(int).to_csv('final_pred_mean_v2.txt', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "6431"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(proba_list).mean(axis=0)[pd.DataFrame(proba_list).mean(axis=0) > 0.9].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "      0         1             2         3         4             5      \\\n0  0.000039  0.000314  3.871243e-07  0.000020  0.002703  4.079205e-09   \n1  0.000017  0.000077  1.575317e-06  0.000005  0.001140  9.342212e-11   \n2  0.002196  0.000041  3.284911e-05  0.019811  0.000009  3.043145e-05   \n3  0.000492  0.018879  2.709725e-06  0.000649  0.000801  3.206604e-05   \n\n      6             7         8         9      ...     44004     44005  \\\n0  0.780691  5.511277e-08  0.033222  0.003809  ...  0.247177  0.797227   \n1  0.819498  5.021664e-06  0.018164  0.000039  ...  0.271288  0.176588   \n2  0.944794  2.235273e-11  0.055784  0.000795  ...  0.852520  0.559663   \n3  0.816199  5.323912e-08  0.087250  0.006023  ...  0.966735  0.325599   \n\n      44006     44007     44008     44009     44010     44011     44012  \\\n0  0.872029  0.000038  0.000002  0.000180  0.899501  0.000058  0.000312   \n1  0.969767  0.000020  0.000012  0.000368  0.667171  0.001109  0.001010   \n2  0.942824  0.000071  0.000112  0.000739  0.659982  0.000639  0.000403   \n3  0.919823  0.001311  0.001053  0.003771  0.734550  0.000516  0.000112   \n\n      44013  \n0  0.163988  \n1  0.760778  \n2  0.242482  \n3  0.575783  \n\n[4 rows x 44014 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>44004</th>\n      <th>44005</th>\n      <th>44006</th>\n      <th>44007</th>\n      <th>44008</th>\n      <th>44009</th>\n      <th>44010</th>\n      <th>44011</th>\n      <th>44012</th>\n      <th>44013</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000039</td>\n      <td>0.000314</td>\n      <td>3.871243e-07</td>\n      <td>0.000020</td>\n      <td>0.002703</td>\n      <td>4.079205e-09</td>\n      <td>0.780691</td>\n      <td>5.511277e-08</td>\n      <td>0.033222</td>\n      <td>0.003809</td>\n      <td>...</td>\n      <td>0.247177</td>\n      <td>0.797227</td>\n      <td>0.872029</td>\n      <td>0.000038</td>\n      <td>0.000002</td>\n      <td>0.000180</td>\n      <td>0.899501</td>\n      <td>0.000058</td>\n      <td>0.000312</td>\n      <td>0.163988</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000017</td>\n      <td>0.000077</td>\n      <td>1.575317e-06</td>\n      <td>0.000005</td>\n      <td>0.001140</td>\n      <td>9.342212e-11</td>\n      <td>0.819498</td>\n      <td>5.021664e-06</td>\n      <td>0.018164</td>\n      <td>0.000039</td>\n      <td>...</td>\n      <td>0.271288</td>\n      <td>0.176588</td>\n      <td>0.969767</td>\n      <td>0.000020</td>\n      <td>0.000012</td>\n      <td>0.000368</td>\n      <td>0.667171</td>\n      <td>0.001109</td>\n      <td>0.001010</td>\n      <td>0.760778</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002196</td>\n      <td>0.000041</td>\n      <td>3.284911e-05</td>\n      <td>0.019811</td>\n      <td>0.000009</td>\n      <td>3.043145e-05</td>\n      <td>0.944794</td>\n      <td>2.235273e-11</td>\n      <td>0.055784</td>\n      <td>0.000795</td>\n      <td>...</td>\n      <td>0.852520</td>\n      <td>0.559663</td>\n      <td>0.942824</td>\n      <td>0.000071</td>\n      <td>0.000112</td>\n      <td>0.000739</td>\n      <td>0.659982</td>\n      <td>0.000639</td>\n      <td>0.000403</td>\n      <td>0.242482</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000492</td>\n      <td>0.018879</td>\n      <td>2.709725e-06</td>\n      <td>0.000649</td>\n      <td>0.000801</td>\n      <td>3.206604e-05</td>\n      <td>0.816199</td>\n      <td>5.323912e-08</td>\n      <td>0.087250</td>\n      <td>0.006023</td>\n      <td>...</td>\n      <td>0.966735</td>\n      <td>0.325599</td>\n      <td>0.919823</td>\n      <td>0.001311</td>\n      <td>0.001053</td>\n      <td>0.003771</td>\n      <td>0.734550</td>\n      <td>0.000516</td>\n      <td>0.000112</td>\n      <td>0.575783</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 44014 columns</p>\n</div>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(proba_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}