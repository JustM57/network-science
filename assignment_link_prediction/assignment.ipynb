{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment — Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Dataset for link prediction (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider link prediction on the [e-mails network](http://snap.stanford.edu/data/email-Eu-core-temporal.html) where nodes are members of a research institution and edges are e-mails given with timestamps. The goal is to predict occurrence of edges in the test time period using information from the train time period only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/email-Eu-core-temporal.txt'\n",
    "open('email-Eu-core-temporal.txt', 'wb').write(requests.get(url).content);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df = pd.read_csv(\n",
    "    'email-Eu-core-temporal.txt', \n",
    "    delimiter=' ', \n",
    "    names=['sender', 'receiver', 'timestamp']\n",
    ")\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the following preprocessing procedure:\n",
    "1. Select edges by given train and test time periods, for example, [0, 1000) is train and [1000, 2000) is test\n",
    "2. Build a _core_ — a connected network where every edge occurs at least $k_\\text{train}$ times in the train time period or at least $k_\\text{test}$ times in the test time period. Let the core be undirected, so occurrences edges (1, 0) and (0, 1) are computed together.\n",
    "3. From the core, select a train set of edges $E_\\text{train}$ that occur for the first time in the train period. All others are included to $E_\\text{test}$.\n",
    "3. Exclude test edges that contain nodes that do not occur in train edges.\n",
    "\n",
    "Write a function `train_test_edges` that takes a pd.DataFrame `email_df` with e-mail network, a tuple with the train time period borders `train_period`, say, (0, 1000), a similar tuple `test_period`, the number of edges occurrences `ktrain` and `ktest`. The function returns two lists with tuples — train and test edges. Every edge is returned of the form where the first node is less than the second, for example [(1, 2), (2, 3)] is ok, but [(2, 1), (3, 2)] is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02465886cf63caec30af195db4cf5b9",
     "grade": false,
     "grade_id": "cell-676bd18fcab3c342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_edges(email_df, train_period, test_period, ktrain, ktest):\n",
    "    \n",
    "    email_temp = email_df.copy()\n",
    "    email_temp = email_temp[\n",
    "        (train_period[0] <= email_temp.timestamp) \n",
    "        & (email_temp.timestamp < test_period[1])\n",
    "    ]\n",
    "    email_temp['from'] = email_temp[['sender', 'receiver']].min(axis=1)\n",
    "    email_temp['to'] = email_temp[['sender', 'receiver']].max(axis=1)\n",
    "    email_temp = email_temp.drop(['sender', 'receiver'], axis=1)\n",
    "    email_temp = email_temp.set_index(['from', 'to'])\n",
    "\n",
    "    email_train = email_temp[email_temp.timestamp < train_period[1]]\n",
    "    email_train = email_train.groupby(['from', 'to']).count()\n",
    "    train_core = email_train[email_train.timestamp >= ktrain].index.tolist()\n",
    "\n",
    "    email_test = email_temp[test_period[0] <= email_temp.timestamp]\n",
    "    email_test = email_test.groupby(['from', 'to']).count()\n",
    "    test_core = email_test[email_test.timestamp >= ktest].index.tolist()\n",
    "    \n",
    "    core = list(set(train_core + test_core))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da565e34660337c25e9701492519f95a",
     "grade": true,
     "grade_id": "cell-a77f2fa764e87595",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_edges, test_edges = train_test_edges(email_df, (1e7, 2e7), (2e7, 2.5e7), 3, 3)\n",
    "_train_edges, _test_edges = np.array(train_edges), np.array(test_edges)\n",
    "assert np.all(_train_edges[:, 0] < _train_edges[:, 1])\n",
    "assert np.all(_test_edges[:, 0] < _test_edges[:, 1])\n",
    "assert len(set(train_edges).intersection(test_edges)) == 0\n",
    "assert _train_edges.shape == (4147, 2)\n",
    "assert _test_edges.shape == (391, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Negative sampling (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, graphs are sparse, so there is a high imbalance between positive (edge exists) and negative classes.\n",
    "To eliminate this problem, we can use the undersampling technique. \n",
    "\n",
    "The `negative_sampling` function samples the unexisted edges from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "550f4ed4fd88b68491c86ca5c1310695",
     "grade": false,
     "grade_id": "cell-2086061e022dc394",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def negative_sampling(train_edges, test_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bc37a39b370339c2b5af888964df05e",
     "grade": true,
     "grade_id": "cell-e71e5d52cb2cae88",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "negatives = negative_sampling(train_edges, test_edges)\n",
    "assert len(negatives) == len(test_edges)\n",
    "assert len(set(negatives) & set(test_edges)) == 0\n",
    "assert len(set(negatives) & set(train_edges)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build test pairs of nodes that contains test and negative edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = np.random.permutation(negatives + test_edges)\n",
    "y_true = [int((u, v) in test_edges) for (u, v) in test_pairs]\n",
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Similarity based algorithm (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity based algorithm:\n",
    "1. Compute similarity matrix for test pairs using a graph on train edges\n",
    "2. Order that pairs in descending of similarity\n",
    "3. Select some threshold and predict links for all pairs above the threshold\n",
    "\n",
    "Write a function `sim_link_prediction` that takes a list with train edges, test pairs and true lables. The function predicts links and returns a tuple with metrics: \n",
    "* two np.arrays: FPR (false positive rate) and TPR (true positive rate) in descending of thresholds obtained by Jaccard coefficient, `nx.jaccard_coefficient`\n",
    "* the same, by Adamic/Adar index, `nx.adamic_adar_index`\n",
    "* the same, by resource allocation index, `nx.resource_allocation_index`\n",
    "\n",
    "_Hint: use `sklearn.metrics.roc_curve`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d3210041a30feb318d3198a746276a6",
     "grade": false,
     "grade_id": "cell-4125af6f7f2c4f56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sim_link_prediction(train_edges, test_pairs, y_true):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(train_edges)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3033a917096b4adfdfce88d37eb630e",
     "grade": true,
     "grade_id": "cell-ea34274053a92113",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "jac, adam, res = sim_link_prediction(\n",
    "    train_edges, \n",
    "    [[537, 120], [267, 630], [37, 730], [251, 887], [773, 97], [327, 506], [280, 687]],\n",
    "    [0, 1, 1, 0, 0, 1, 0]\n",
    ")\n",
    "assert jac[0].shape == jac[1].shape\n",
    "assert adam[0].shape == adam[1].shape\n",
    "assert res[0].shape == res[1].shape\n",
    "assert round(auc(jac[0], jac[1]), 4) == 1\n",
    "assert round(auc(adam[0], adam[1]), 4) == 0.8333\n",
    "assert round(auc(res[0], res[1]), 4) == 0.9167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at ROC AUC curve to compare similaritites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac, adam, res = sim_link_prediction(train_edges, test_pairs, y_true)\n",
    "plt.figure(figsize=(10, 6))\n",
    "cases = [[jac[0], jac[1], 'Jaccard'], \n",
    "         [adam[0], adam[1], 'Adamic/Adar'], \n",
    "         [res[0], res[1], 'Resource alloc.']]\n",
    "for fpr, tpr, label in cases:\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Reconstruction adjacency matrix using SVD (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the node classification task, node embeddings could be helpful in the link prediction problem. A simple way to obtain similarity score for link prediction is to reconstruct adjacency matrix using dot product of truncated SVD node embeddings $$\\tilde A_{ij} = \\langle e_i, e_j \\rangle$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(np.max(train_edges) + 1))\n",
    "G.add_edges_from(train_edges)\n",
    "A = nx.to_numpy_array(G)\n",
    "model = TruncatedSVD(n_components=4, random_state=0)\n",
    "emb = model.fit_transform(A)\n",
    "emb.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `dot_product_scores` that takes node embeddings, test pairs, returns reconstructed scores for test pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2eb68405b8b45411bfd09ce363857f9",
     "grade": false,
     "grade_id": "cell-e3728903f481eed8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dot_product_scores(emb, test_pairs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6109a9d205365fc16f2d6b5d1cd64cc4",
     "grade": true,
     "grade_id": "cell-d7b6b9f3c20abc71",
     "locked": true,
     "points": 3.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = dot_product_scores(emb, test_pairs)\n",
    "assert scores.shape == (782, )\n",
    "fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "assert auc(fpr, tpr) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, lw=2, \n",
    "         label='{}, AUC={:.4f}'.format('Adjacency SVD', auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Edge emdedding (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will solve the link prediction as classification task on edge embeddings. Let us compare several techniques of edge embedding calculation from the [paper](https://peerj.com/articles/cs-172/#table-2). We will compare the different vector aggregations as features for `sklearn.linear_model.LogisticRegression` for classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(np.max(train_edges) + 1))\n",
    "G.add_edges_from(train_edges)\n",
    "A = nx.to_numpy_array(G)\n",
    "model = TruncatedSVD(n_components=4, n_iter=100)\n",
    "emb = model.fit_transform(A)\n",
    "emb.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All following functions should return np.array with embeddings of edges from edges param. Average operator is simple elementwise average of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f462d94128ad39168d5c1651115bbc",
     "grade": false,
     "grade_id": "cell-26173c0c479136b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def average_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc61e04094cfdf04d4a4357b8afd1c8f",
     "grade": true,
     "grade_id": "cell-eaefe9963be3c2d0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    average_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[ 0.55, -0.14, -0.26, -0.13]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard product is an elementwise product of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dca0930cdead56210ac1d3ed38a1da2d",
     "grade": false,
     "grade_id": "cell-f662abe3d0579575",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hadamard_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "440462a8cfd997877c13c9929e7b5772",
     "grade": true,
     "grade_id": "cell-db04660af550adc7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    hadamard_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.2 , 0.02, 0.06, 0.02]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L1 is a absolute of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6257c4c2eb4680600acc5867ddba862",
     "grade": false,
     "grade_id": "cell-5e44fbdf6a64715d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l1_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec862cd555da0fe6278aa191d0855aff",
     "grade": true,
     "grade_id": "cell-f2c7878af729fcb5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    weighted_l1_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.64, 0.07, 0.16, 0.11]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L2 is a square of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c81ca1cb9941e2216cb755dcb6b1d6df",
     "grade": false,
     "grade_id": "cell-0f7f5b3663337374",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l2_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e6fb9bbeff70f61d8b1c3974464baee",
     "grade": true,
     "grade_id": "cell-957e1faea3e9127d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    weighted_l2_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.41, 0.01, 0.03, 0.01]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a absolute of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91862e3967ecf48894e45a0adf8af354",
     "grade": false,
     "grade_id": "cell-9663ec4df9dbb751",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l1_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cbefe98af2a1b6b2376f799b866d939",
     "grade": true,
     "grade_id": "cell-fe7f124fa265373c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    neighbor_weighted_l1_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[2.17, 0.18, 0.34, 0.2 ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a square of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fe87475999a79b39066260f060f80ca",
     "grade": false,
     "grade_id": "cell-cb55782d381636cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l2_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae389acc9d3af30a4e104f746d1f56f",
     "grade": true,
     "grade_id": "cell-b3ea740a7d8e7422",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    neighbor_weighted_l2_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[4.69, 0.03, 0.12, 0.04]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(test_pairs) * 0.8)\n",
    "res = {}\n",
    "for name, operator in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    edge_emb = operator(G, emb, test_pairs)\n",
    "    lr.fit(edge_emb[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(edge_emb[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Walklets model (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walklets (Perozzi, Kulkarni & Skiena, 2016) use a weighted combination of embeddings of powers of adjacency matrix $A$, $A^2$, …, $A^k$ to reduce the bias of Deepwalk for low-order proximities, and approximates computing $A^i$ by skipping nodes using short random walks (Perozzi et al., 2017). Walklets captures multiple scales of relationships between vertices in a graph. The method itself is scalable, and can be run on graphs with millions of vertices.\n",
    "\n",
    "Firstly, we need to sample some random walks. You can take this function from the DeepWalk task of the node classification assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "999ee0e8c86557f2448d8a05ed3037b7",
     "grade": false,
     "grade_id": "cell-15f0d69f9b2d6d42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def random_walks(G, n_walks, path_length):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ed1cd07b868701427efb505f8729593",
     "grade": true,
     "grade_id": "cell-1832ce04312cc4a9",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "walks = random_walks(nx.karate_club_graph(), 10, 5)\n",
    "\n",
    "assert walks.shape == (34*10, 5)\n",
    "for i, j in zip(walks[0, :-1], walks[0, 1:]):\n",
    "    assert nx.karate_club_graph().has_edge(i, j)\n",
    "assert np.all(walks[:, 0] == np.repeat(np.arange(34), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to DeepWalk, we model the network through a series of truncated random walks started at each node. However, we make a key change to the sampling procedure. Specifically, we choose to skip some of the the nodes in the random walk. In this way, we form a set of relationships which are sampled from successively higher powers of A.\n",
    "\n",
    "Function `skip_steps` separates a random walk `walk` on the several walks with `n_steps` steps between nodes. It returns list of lists with random walks with skips steps, look at asserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27ca9a612f54a5c3a87db00e84c97339",
     "grade": false,
     "grade_id": "cell-94ff0150d47246f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def skip_steps(walk, n_steps):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb15f7e8e35cb4d34d398d34f820814",
     "grade": true,
     "grade_id": "cell-cf0f3db3a583d5c7",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "walk = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "assert skip_steps(walk, 2) == [[0, 2, 4, 6, 8], [1, 3, 5, 7, 9], [2, 4, 6, 8]]\n",
    "assert skip_steps(walk, 3) == [[0, 3, 6, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9]]\n",
    "\n",
    "skipped = skip_steps(walks[0], 2)\n",
    "assert len(skipped) == 3\n",
    "assert len(skipped[1]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare walklets as follows: skip steps in each random walk and union them into a single list. Write a function `generate_walklets` that takes input random walks and number of steps, return a dataset with skipped random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50b5c40024da6b6fafc6f52cc67f2405",
     "grade": false,
     "grade_id": "cell-37b67a0e80a3f3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_walklets(input_walks, n_steps):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30dfa61724fb749e61147204b78e1b95",
     "grade": true,
     "grade_id": "cell-92b5dc686df237f5",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "example_walks = [[0, 2, 4, 6, 8], [1, 3, 5, 7, 9], [2, 4, 6, 8]]\n",
    "assert (\n",
    "    generate_walklets(example_walks, 2) == \n",
    "    [[0, 4, 8], [2, 6], [4, 8], [1, 5, 9], [3, 7], [5, 9], [2, 6], [4, 8], [6]]\n",
    ")\n",
    "\n",
    "walklets = generate_walklets(walks, 2)\n",
    "assert len(walklets) == 1020\n",
    "assert len(walklets[1]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train embedding you need to know the set of nodes, sampled random walks without skips, size of the maximal desired skip (window_size) and dimension of embedding for the one skip.\n",
    "\n",
    "The function `train_embedding` should work as follows:\n",
    "For each skip_length between `1` and `window_size + 1`\n",
    "1. Create dataset with splits\n",
    "2. Train Word2Vec model on the created walklets with given vector_size, min_count=1, epochs=10 and window=1.\n",
    "3. Save embeddings for the given step\n",
    "\n",
    "After all iterations you need to take a mean of received embeddings for a node from each step. Finally, we return np.array with embeddings ordered by the id of node, if node id has no embedding, then use `np.zeros(vector_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71fffccd20964f4663aa4d08567059c3",
     "grade": false,
     "grade_id": "cell-2dd995c4351416bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_embedding(nodes, walks, window_size, vector_size):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302b7fb73d64e62624e2d87c1c39a275",
     "grade": true,
     "grade_id": "cell-caa5c52f4660af25",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "G = nx.Graph(train_edges)\n",
    "nodes = np.arange(np.max(train_edges) + 1)\n",
    "walks = random_walks(G, n_walks=5, path_length=10)\n",
    "emb = train_embedding(nodes, walks, window_size=3, vector_size=8)\n",
    "assert emb.shape == (1005, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e8dc6e7dfe8f09ed4a7d9e5b707aa33",
     "grade": true,
     "grade_id": "cell-98cf8c96d4d4ad52",
     "locked": true,
     "points": 0.7999999999999998,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(test_pairs) * 0.8)\n",
    "res = {}\n",
    "for name, operator in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    edge_emb = operator(G, emb, test_pairs)\n",
    "    lr.fit(edge_emb[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(edge_emb[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "assert auc(fpr, tpr) > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
